import { Injectable, Logger } from '@nestjs/common';
import { MetadataService } from '../metadata/metadata.service.js';
import { ConnectionsService } from '../connections/connections.service.js';
import { SchemaDiffService } from '../schema/schema-diff.service.js';
import { parseColumnArray } from '../schema/sql-generator.js';
import type {
    InstanceGroup,
    InstanceGroupSyncStatus,
    InstanceGroupTargetStatus,
    ConnectionConfig,
    DatabaseEngine,
} from '@dbnexus/shared';

// Helper to quote identifiers based on database engine
function quoteIdentifier(name: string, engine: DatabaseEngine): string {
    if (engine === 'mysql' || engine === 'mariadb') {
        return `\`${name}\``;
    }
    return `"${name}"`;
}

function quoteTableRef(schema: string, table: string, engine: DatabaseEngine): string {
    if (engine === 'sqlite') {
        return quoteIdentifier(table, engine);
    }
    return `${quoteIdentifier(schema, engine)}.${quoteIdentifier(table, engine)}`;
}

// Helper to get placeholder syntax for parameterized queries
function getPlaceholder(index: number, engine: DatabaseEngine): string {
    if (engine === 'mysql' || engine === 'mariadb') {
        return '?';
    }
    return `$${index}`;
}

// Helper to check if a column type is JSON/JSONB
function isJsonColumn(dataType: string): boolean {
    const typeLower = dataType.toLowerCase();
    return typeLower === 'json' || typeLower === 'jsonb';
}

// Helper to serialize a value for insertion, handling JSON columns
function serializeValueForInsert(value: unknown, dataType: string): unknown {
    if (value === null || value === undefined) {
        return value;
    }

    // JSON/JSONB columns need to be stringified if they're objects
    if (isJsonColumn(dataType) && typeof value === 'object') {
        return JSON.stringify(value);
    }

    return value;
}

// Helper to check if a column is auto-generated (serial, identity, auto_increment)
function isAutoGeneratedColumn(column: {
    name: string;
    dataType: string;
    defaultValue: string | null;
    isPrimaryKey?: boolean;
}): boolean {
    const dataTypeLower = column.dataType.toLowerCase();
    const defaultLower = (column.defaultValue || '').toLowerCase();

    // Check data type for serial/identity
    if (dataTypeLower.includes('serial') || dataTypeLower.includes('identity')) {
        return true;
    }

    // Check default value for nextval (PostgreSQL sequences) - this is the main indicator for serial columns
    if (defaultLower.includes('nextval(') || defaultLower.includes("nextval('")) {
        return true;
    }

    // Check for auto_increment in MySQL/MariaDB (usually in default or extra info)
    if (defaultLower.includes('auto_increment')) {
        return true;
    }

    // Check for generated columns (PostgreSQL 12+)
    if (defaultLower.includes('generated')) {
        return true;
    }

    return false;
}

export interface TableDataDiff {
    table: string;
    schema: string;
    sourceCount: number;
    targetCount: number;
    missingInTarget: number;
    missingInSource: number;
    different: number;
}

export interface DataSyncResult {
    table: string;
    schema: string;
    inserted: number;
    updated: number;
    deleted: number;
    errors: string[];
}

@Injectable()
export class SyncService {
    private readonly logger = new Logger(SyncService.name);

    constructor(
        private readonly metadataService: MetadataService,
        private readonly connectionsService: ConnectionsService,
        private readonly schemaDiffService: SchemaDiffService
    ) { }

    /**
     * Get sync status for an instance group
     */
    async getGroupSyncStatus(groupId: string): Promise<InstanceGroupSyncStatus | null> {
        const group = this.metadataService.databaseGroupRepository.findById(groupId);
        if (!group) return null;

        const connections = this.metadataService.connectionRepository.findByGroup(groupId);
        if (connections.length === 0) {
            return {
                groupId: group.id,
                groupName: group.name,
                sourceConnectionId: group.sourceConnectionId,
                sourceConnectionName: group.sourceConnectionName,
                targets: [],
                lastChecked: new Date(),
            };
        }

        // If no source is set, return unchecked status
        if (!group.sourceConnectionId) {
            return {
                groupId: group.id,
                groupName: group.name,
                targets: connections.map((conn) => ({
                    connectionId: conn.id,
                    connectionName: conn.name,
                    schemaStatus: 'unchecked' as const,
                    dataStatus: 'unchecked' as const,
                })),
                lastChecked: new Date(),
            };
        }

        const sourceConnection = connections.find((c) => c.id === group.sourceConnectionId);
        const targetConnections = connections.filter((c) => c.id !== group.sourceConnectionId);

        const targets: InstanceGroupTargetStatus[] = [];

        for (const target of targetConnections) {
            const status = await this.checkTargetStatus(group, sourceConnection!, target);
            targets.push(status);
        }

        return {
            groupId: group.id,
            groupName: group.name,
            sourceConnectionId: group.sourceConnectionId,
            sourceConnectionName: sourceConnection?.name,
            targets,
            lastChecked: new Date(),
        };
    }

    /**
     * Check sync status for a single target connection in a group
     */
    async checkSingleTargetStatus(
        groupId: string,
        targetConnectionId: string
    ): Promise<InstanceGroupTargetStatus | null> {
        const group = this.metadataService.databaseGroupRepository.findById(groupId);
        if (!group || !group.sourceConnectionId) return null;

        const connections = this.metadataService.connectionRepository.findByGroup(groupId);
        const sourceConnection = connections.find((c) => c.id === group.sourceConnectionId);
        const targetConnection = connections.find((c) => c.id === targetConnectionId);

        if (!sourceConnection || !targetConnection) return null;

        return this.checkTargetStatus(group, sourceConnection, targetConnection);
    }

    /**
     * Check sync status for a single target - includes full diff data
     */
    private async checkTargetStatus(
        group: InstanceGroup,
        source: ConnectionConfig,
        target: ConnectionConfig
    ): Promise<InstanceGroupTargetStatus> {
        const status: InstanceGroupTargetStatus = {
            connectionId: target.id,
            connectionName: target.name,
            schemaStatus: 'unchecked',
            dataStatus: 'unchecked',
        };

        // Use group's syncTargetSchema if set, otherwise fall back to connection's defaultSchema
        // For MySQL/MariaDB, the database name is the schema
        const getDefaultSchema = (conn: ConnectionConfig) => {
            if (conn.engine === 'mysql' || conn.engine === 'mariadb') {
                return conn.database;
            }
            return conn.defaultSchema || 'public';
        };
        const sourceSchema = group.syncTargetSchema || getDefaultSchema(source);
        const targetSchema = group.syncTargetSchema || getDefaultSchema(target);

        // Check schema if enabled
        if (group.syncSchema) {
            try {
                const schemaDiff = await this.schemaDiffService.compareSchemas(
                    source.id,
                    target.id,
                    sourceSchema,
                    targetSchema
                );

                if (schemaDiff.items.length === 0) {
                    status.schemaStatus = 'in_sync';
                    status.schemaDiffCount = 0;
                } else {
                    status.schemaStatus = 'out_of_sync';
                    status.schemaDiffCount = schemaDiff.items.length;
                    // Include full schema diff data
                    status.schemaDiff = schemaDiff;
                    // Generate migration SQL from the diff
                    try {
                        const migrationSql = this.schemaDiffService.getMigrationSql(schemaDiff);
                        status.migrationSql = migrationSql;
                    } catch (sqlError) {
                        this.logger.warn(`Failed to generate migration SQL: ${sqlError}`);
                    }
                }
            } catch (error) {
                status.schemaStatus = 'error';
                status.error = error instanceof Error ? error.message : 'Schema check failed';
            }
        }

        // Check data if enabled
        if (group.syncData) {
            try {
                const dataDiff = await this.getTableRowCounts(source.id, target.id, sourceSchema);
                const outOfSync = dataDiff.filter(
                    (d) => d.sourceCount !== d.targetCount || d.missingInTarget > 0
                );

                if (outOfSync.length === 0) {
                    status.dataStatus = 'in_sync';
                    status.dataDiffSummary = 'All tables in sync';
                } else {
                    status.dataStatus = 'out_of_sync';
                    status.dataDiffSummary = `${outOfSync.length} table(s) out of sync`;
                }
                // Include full data diff
                status.dataDiff = dataDiff.map((d) => ({
                    table: d.table,
                    sourceCount: d.sourceCount,
                    targetCount: d.targetCount,
                    missingInTarget: d.missingInTarget,
                    missingInSource: d.missingInSource,
                }));
            } catch (error) {
                status.dataStatus = 'error';
                status.error =
                    (status.error ? status.error + '; ' : '') +
                    (error instanceof Error ? error.message : 'Data check failed');
            }
        }

        return status;
    }

    /**
     * Get row counts for all tables in source vs target
     */
    async getTableRowCounts(
        sourceConnectionId: string,
        targetConnectionId: string,
        schema: string = 'public'
    ): Promise<TableDataDiff[]> {
        const sourceConnection = this.connectionsService.findById(sourceConnectionId);
        const targetConnection = this.connectionsService.findById(targetConnectionId);
        const sourceConnector = await this.connectionsService.getConnector(sourceConnectionId);
        const targetConnector = await this.connectionsService.getConnector(targetConnectionId);

        // Get tables from source
        const sourceTables = await sourceConnector.getTables(schema);
        const results: TableDataDiff[] = [];

        for (const table of sourceTables) {
            try {
                const sourceTableRef = quoteTableRef(schema, table.name, sourceConnection.engine);
                const targetTableRef = quoteTableRef(schema, table.name, targetConnection.engine);

                const sourceCountResult = await sourceConnector.query(
                    `SELECT COUNT(*) as count FROM ${sourceTableRef}`
                );
                const targetCountResult = await targetConnector.query(
                    `SELECT COUNT(*) as count FROM ${targetTableRef}`
                );

                const sourceCount = Number(sourceCountResult.rows[0]?.count ?? 0);
                const targetCount = Number(targetCountResult.rows[0]?.count ?? 0);

                results.push({
                    table: table.name,
                    schema,
                    sourceCount,
                    targetCount,
                    missingInTarget: Math.max(0, sourceCount - targetCount),
                    missingInSource: Math.max(0, targetCount - sourceCount),
                    different: 0, // Would need primary key comparison to determine
                });
            } catch (error) {
                this.logger.warn(`Failed to get row count for ${schema}.${table.name}: ${error}`);
            }
        }

        return results;
    }

    /**
     * Get detailed data diff for a specific table
     */
    async getTableDataDiff(
        sourceConnectionId: string,
        targetConnectionId: string,
        schema: string,
        table: string,
        primaryKeyColumns: string[] | string
    ): Promise<{
        missingInTarget: Record<string, unknown>[];
        missingInSource: Record<string, unknown>[];
        different: { source: Record<string, unknown>; target: Record<string, unknown> }[];
    }> {
        // Parse primaryKeyColumns in case it's a PostgreSQL array string
        const pkColumns = parseColumnArray(primaryKeyColumns);
        if (pkColumns.length === 0) {
            throw new Error('No primary key columns provided');
        }

        const sourceConnection = this.connectionsService.findById(sourceConnectionId);
        const targetConnection = this.connectionsService.findById(targetConnectionId);
        const sourceConnector = await this.connectionsService.getConnector(sourceConnectionId);
        const targetConnector = await this.connectionsService.getConnector(targetConnectionId);

        const sourceTableRef = quoteTableRef(schema, table, sourceConnection.engine);
        const targetTableRef = quoteTableRef(schema, table, targetConnection.engine);
        const sourceOrderBy = pkColumns
            .map((c) => quoteIdentifier(c, sourceConnection.engine))
            .join(', ');
        const targetOrderBy = pkColumns
            .map((c) => quoteIdentifier(c, targetConnection.engine))
            .join(', ');

        // Get all rows from both
        const sourceResult = await sourceConnector.query(
            `SELECT * FROM ${sourceTableRef} ORDER BY ${sourceOrderBy}`
        );
        const targetResult = await targetConnector.query(
            `SELECT * FROM ${targetTableRef} ORDER BY ${targetOrderBy}`
        );

        // Create maps by primary key
        const getPkValue = (row: Record<string, unknown>) =>
            pkColumns.map((c) => String(row[c])).join('|');

        const sourceMap = new Map<string, Record<string, unknown>>();
        const targetMap = new Map<string, Record<string, unknown>>();

        for (const row of sourceResult.rows) {
            sourceMap.set(getPkValue(row), row);
        }
        for (const row of targetResult.rows) {
            targetMap.set(getPkValue(row), row);
        }

        const missingInTarget: Record<string, unknown>[] = [];
        const missingInSource: Record<string, unknown>[] = [];
        const different: { source: Record<string, unknown>; target: Record<string, unknown> }[] =
            [];

        // Find missing in target and different
        for (const [pk, sourceRow] of sourceMap) {
            const targetRow = targetMap.get(pk);
            if (!targetRow) {
                missingInTarget.push(sourceRow);
            } else if (JSON.stringify(sourceRow) !== JSON.stringify(targetRow)) {
                different.push({ source: sourceRow, target: targetRow });
            }
        }

        // Find missing in source
        for (const [pk, targetRow] of targetMap) {
            if (!sourceMap.has(pk)) {
                missingInSource.push(targetRow);
            }
        }

        return { missingInTarget, missingInSource, different };
    }

    /**
     * Sync data from source to target for a specific table
     */
    async syncTableData(
        sourceConnectionId: string,
        targetConnectionId: string,
        schema: string,
        table: string,
        primaryKeyColumns: string[],
        options: {
            insertMissing?: boolean;
            updateDifferent?: boolean;
            deleteExtra?: boolean;
        } = { insertMissing: true, updateDifferent: true, deleteExtra: false }
    ): Promise<DataSyncResult> {
        const result: DataSyncResult = {
            table,
            schema,
            inserted: 0,
            updated: 0,
            deleted: 0,
            errors: [],
        };

        const targetConnection = this.connectionsService.findById(targetConnectionId);
        const targetConnector = await this.connectionsService.getConnector(targetConnectionId);

        // Parse primaryKeyColumns in case it's a PostgreSQL array string
        const parsedPkColumns = parseColumnArray(primaryKeyColumns);

        // Auto-detect primary keys if not provided or only default 'id' is passed
        let effectivePkColumns = parsedPkColumns;
        if (
            parsedPkColumns.length === 0 ||
            (parsedPkColumns.length === 1 && parsedPkColumns[0] === 'id')
        ) {
            const tableSchema = await targetConnector.getTableSchema(schema, table);
            const schemaPks = parseColumnArray(tableSchema.primaryKey);
            if (schemaPks.length > 0) {
                effectivePkColumns = schemaPks;
                this.logger.debug(
                    `Auto-detected primary keys for ${schema}.${table}: ${effectivePkColumns.join(', ')}`
                );
            } else {
                // Fall back to 'id' if no PK detected
                effectivePkColumns = ['id'];
                this.logger.warn(
                    `No primary key found for ${schema}.${table}, falling back to 'id'`
                );
            }
        }

        // Log sync start to audit (sync_runs table requires a sync_config, so for ad-hoc syncs we only use audit log)
        const syncStartEntry = this.metadataService.auditLogRepository.create({
            action: 'data_sync_started',
            entityType: 'table',
            entityId: `${schema}.${table}`,
            connectionId: targetConnectionId,
            details: {
                sourceConnectionId,
                targetConnectionId,
                schema,
                table,
                primaryKeyColumns: effectivePkColumns,
                options,
            },
        });
        const diff = await this.getTableDataDiff(
            sourceConnectionId,
            targetConnectionId,
            schema,
            table,
            effectivePkColumns
        );

        // Get column info (may have been fetched already for PK detection, but needed for columns)
        const tableSchema = await targetConnector.getTableSchema(schema, table);
        const columns = tableSchema.columns.map((c) => c.name);
        const columnTypes = new Map(tableSchema.columns.map((c) => [c.name, c.dataType]));
        // Exclude auto-generated columns from inserts
        const insertableColumns = tableSchema.columns
            .filter((c) => !isAutoGeneratedColumn(c))
            .map((c) => c.name);
        const engine = targetConnection.engine;
        const tableRef = quoteTableRef(schema, table, engine);

        // Helper to serialize values for this table
        const serializeValue = (colName: string, value: unknown) => {
            const dataType = columnTypes.get(colName) || '';
            return serializeValueForInsert(value, dataType);
        };

        // Insert missing rows
        if (options.insertMissing && diff.missingInTarget.length > 0) {
            for (const row of diff.missingInTarget) {
                try {
                    const values = insertableColumns.map((c) => serializeValue(c, row[c]));
                    const placeholders = insertableColumns
                        .map((_, i) => getPlaceholder(i + 1, engine))
                        .join(', ');
                    const quotedColumns = insertableColumns
                        .map((c) => quoteIdentifier(c, engine))
                        .join(', ');
                    await targetConnector.execute(
                        `INSERT INTO ${tableRef} (${quotedColumns}) VALUES (${placeholders})`,
                        values
                    );
                    result.inserted++;
                } catch (error) {
                    result.errors.push(
                        `Insert failed: ${error instanceof Error ? error.message : String(error)}`
                    );
                }
            }
        }

        // Update different rows
        if (options.updateDifferent && diff.different.length > 0) {
            for (const { source } of diff.different) {
                try {
                    const nonPkColumns = columns.filter((c) => !effectivePkColumns.includes(c));
                    const setClause = nonPkColumns
                        .map(
                            (c, i) =>
                                `${quoteIdentifier(c, engine)} = ${getPlaceholder(i + 1, engine)}`
                        )
                        .join(', ');
                    const whereClause = effectivePkColumns
                        .map(
                            (c, i) =>
                                `${quoteIdentifier(c, engine)} = ${getPlaceholder(nonPkColumns.length + i + 1, engine)}`
                        )
                        .join(' AND ');
                    const values = [
                        ...nonPkColumns.map((c) => serializeValue(c, source[c])),
                        ...effectivePkColumns.map((c) => serializeValue(c, source[c])),
                    ];

                    await targetConnector.execute(
                        `UPDATE ${tableRef} SET ${setClause} WHERE ${whereClause}`,
                        values
                    );
                    result.updated++;
                } catch (error) {
                    result.errors.push(
                        `Update failed: ${error instanceof Error ? error.message : String(error)}`
                    );
                }
            }
        }

        // Delete extra rows
        if (options.deleteExtra && diff.missingInSource.length > 0) {
            for (const row of diff.missingInSource) {
                try {
                    const whereClause = effectivePkColumns
                        .map(
                            (c, i) =>
                                `${quoteIdentifier(c, engine)} = ${getPlaceholder(i + 1, engine)}`
                        )
                        .join(' AND ');
                    const values = effectivePkColumns.map((c) => row[c]);

                    await targetConnector.execute(
                        `DELETE FROM ${tableRef} WHERE ${whereClause}`,
                        values
                    );
                    result.deleted++;
                } catch (error) {
                    result.errors.push(
                        `Delete failed: ${error instanceof Error ? error.message : String(error)}`
                    );
                }
            }
        }

        // Log sync completion to audit
        this.metadataService.auditLogRepository.create({
            action: result.errors.length > 0 ? 'data_sync_failed' : 'data_sync_completed',
            entityType: 'table',
            entityId: `${schema}.${table}`,
            connectionId: targetConnectionId,
            details: {
                syncStartId: syncStartEntry.id,
                sourceConnectionId,
                table,
                schema,
                primaryKeyColumns: effectivePkColumns,
                inserted: result.inserted,
                updated: result.updated,
                deleted: result.deleted,
                errors: result.errors,
            },
        });

        return result;
    }

    /**
     * Sync specific rows from source to target by primary key values
     */
    async syncRows(
        sourceConnectionId: string,
        targetConnectionId: string,
        sourceSchema: string,
        targetSchema: string,
        table: string,
        rowIds: Record<string, unknown>[], // Array of primary key value objects
        primaryKeyColumns: string[],
        mode: 'insert' | 'upsert' = 'upsert'
    ): Promise<{ inserted: number; updated: number; errors: string[] }> {
        const result = { inserted: 0, updated: 0, errors: [] as string[] };

        if (rowIds.length === 0 || primaryKeyColumns.length === 0) {
            return result;
        }

        const sourceConnection = this.connectionsService.findById(sourceConnectionId);
        const targetConnection = this.connectionsService.findById(targetConnectionId);
        const sourceConnector = await this.connectionsService.getConnector(sourceConnectionId);
        const targetConnector = await this.connectionsService.getConnector(targetConnectionId);

        const sourceEngine = sourceConnection.engine;
        const targetEngine = targetConnection.engine;
        const sourceTableRef = quoteTableRef(sourceSchema, table, sourceEngine);
        const targetTableRef = quoteTableRef(targetSchema, table, targetEngine);

        // Get column info from target
        const tableSchema = await targetConnector.getTableSchema(targetSchema, table);
        const columns = tableSchema.columns.map((c) => c.name);
        const columnTypes = new Map(tableSchema.columns.map((c) => [c.name, c.dataType]));
        // Exclude auto-generated columns from inserts
        const insertableColumns = tableSchema.columns
            .filter((c) => !isAutoGeneratedColumn(c))
            .map((c) => c.name);

        // Helper to serialize values for this table
        const serializeValue = (colName: string, value: unknown) => {
            const dataType = columnTypes.get(colName) || '';
            return serializeValueForInsert(value, dataType);
        };

        for (const rowId of rowIds) {
            try {
                // Fetch the full row from source
                const sourceWhereClause = primaryKeyColumns
                    .map(
                        (c, i) =>
                            `${quoteIdentifier(c, sourceEngine)} = ${getPlaceholder(i + 1, sourceEngine)}`
                    )
                    .join(' AND ');
                const pkValues = primaryKeyColumns.map((c) => rowId[c]);

                const sourceResult = await sourceConnector.query(
                    `SELECT * FROM ${sourceTableRef} WHERE ${sourceWhereClause} LIMIT 1`,
                    pkValues
                );

                if (sourceResult.rows.length === 0) {
                    result.errors.push(`Row not found in source: ${JSON.stringify(rowId)}`);
                    continue;
                }

                const row = sourceResult.rows[0] as Record<string, unknown>;

                // Check if row exists in target
                const targetWhereClause = primaryKeyColumns
                    .map(
                        (c, i) =>
                            `${quoteIdentifier(c, targetEngine)} = ${getPlaceholder(i + 1, targetEngine)}`
                    )
                    .join(' AND ');

                const existsResult = await targetConnector.query(
                    `SELECT 1 FROM ${targetTableRef} WHERE ${targetWhereClause} LIMIT 1`,
                    pkValues
                );
                const exists = existsResult.rows.length > 0;

                if (exists && mode === 'upsert') {
                    // Update existing row - exclude auto-generated and primary key columns
                    const updatableColumns = columns.filter(
                        (c) =>
                            !primaryKeyColumns.includes(c) &&
                            !isAutoGeneratedColumn(
                                tableSchema.columns.find((col) => col.name === c)!
                            )
                    );
                    if (updatableColumns.length > 0) {
                        const setClause = updatableColumns
                            .map(
                                (c, i) =>
                                    `${quoteIdentifier(c, targetEngine)} = ${getPlaceholder(i + 1, targetEngine)}`
                            )
                            .join(', ');
                        const updateWhereClause = primaryKeyColumns
                            .map(
                                (c, i) =>
                                    `${quoteIdentifier(c, targetEngine)} = ${getPlaceholder(updatableColumns.length + i + 1, targetEngine)}`
                            )
                            .join(' AND ');
                        const updateValues = [
                            ...updatableColumns.map((c) => serializeValue(c, row[c])),
                            ...primaryKeyColumns.map((c) => serializeValue(c, row[c])),
                        ];

                        await targetConnector.execute(
                            `UPDATE ${targetTableRef} SET ${setClause} WHERE ${updateWhereClause}`,
                            updateValues
                        );
                        result.updated++;
                    }
                } else if (!exists) {
                    // Insert new row - use only insertable columns that have values
                    const rowColumns = insertableColumns.filter((c) => row[c] !== undefined);
                    const values = rowColumns.map((c) => serializeValue(c, row[c]));
                    const placeholders = rowColumns
                        .map((_, i) => getPlaceholder(i + 1, targetEngine))
                        .join(', ');
                    const quotedColumns = rowColumns
                        .map((c) => quoteIdentifier(c, targetEngine))
                        .join(', ');

                    await targetConnector.execute(
                        `INSERT INTO ${targetTableRef} (${quotedColumns}) VALUES (${placeholders})`,
                        values
                    );
                    result.inserted++;
                }
            } catch (error) {
                result.errors.push(
                    `Row sync failed for ${JSON.stringify(rowId)}: ${error instanceof Error ? error.message : String(error)}`
                );
            }
        }

        return result;
    }

    /**
     * Get groups with sync enabled for dashboard
     */
    getGroupsWithSyncEnabled(): InstanceGroup[] {
        return this.metadataService.databaseGroupRepository.findWithSyncEnabled();
    }

    /**
     * Get tables in dependency order (tables with no FK dependencies first)
     * This is used for dump & restore to ensure data is inserted in correct order
     */
    async getTablesInDependencyOrder(connectionId: string, schema: string): Promise<string[]> {
        const connector = await this.connectionsService.getConnector(connectionId);
        const tables = await connector.getTables(schema);

        // Build dependency graph
        const dependencies = new Map<string, Set<string>>(); // table -> tables it depends on
        const allTables = new Set<string>();

        for (const table of tables) {
            allTables.add(table.name);
            dependencies.set(table.name, new Set());
        }

        // Get foreign keys for each table to build dependencies
        for (const table of tables) {
            try {
                const tableSchema = await connector.getTableSchema(schema, table.name);
                for (const fk of tableSchema.foreignKeys) {
                    // Only add dependency if the referenced table is in our schema
                    if (fk.referencedSchema === schema && allTables.has(fk.referencedTable)) {
                        // This table depends on the referenced table (don't add self-references)
                        if (fk.referencedTable !== table.name) {
                            dependencies.get(table.name)!.add(fk.referencedTable);
                        }
                    }
                }
            } catch (error) {
                this.logger.warn(`Failed to get schema for ${schema}.${table.name}: ${error}`);
            }
        }

        // Topological sort using Kahn's algorithm
        const result: string[] = [];
        const inDegree = new Map<string, number>();

        // Calculate in-degrees
        for (const table of allTables) {
            inDegree.set(table, 0);
        }
        for (const [, deps] of dependencies) {
            for (const dep of deps) {
                inDegree.set(dep, (inDegree.get(dep) || 0) + 1);
            }
        }

        // Start with tables that have no dependencies
        const queue: string[] = [];
        for (const [table, degree] of inDegree) {
            if (degree === 0) {
                queue.push(table);
            }
        }

        while (queue.length > 0) {
            const table = queue.shift()!;
            result.push(table);

            // For each table that depends on this one, reduce its in-degree
            for (const [depTable, deps] of dependencies) {
                if (deps.has(table)) {
                    const newDegree = (inDegree.get(depTable) || 1) - 1;
                    inDegree.set(depTable, newDegree);
                    if (newDegree === 0) {
                        queue.push(depTable);
                    }
                }
            }
        }

        // Handle circular dependencies - add remaining tables
        for (const table of allTables) {
            if (!result.includes(table)) {
                result.push(table);
            }
        }

        // Reverse so that tables with dependencies come after their dependencies
        return result.reverse();
    }

    /**
     * Dump and restore all data from source to target schema
     * This handles foreign key constraints by:
     * 1. Disabling FK checks (if supported)
     * 2. Truncating all tables in reverse dependency order
     * 3. Copying all data in dependency order
     * 4. Re-enabling FK checks
     */
    async dumpAndRestore(
        sourceConnectionId: string,
        targetConnectionId: string,
        schema: string,
        options: {
            truncateTarget?: boolean;
            tables?: string[]; // Optional list of specific tables
        } = { truncateTarget: true }
    ): Promise<{
        success: boolean;
        tablesProcessed: number;
        rowsCopied: number;
        errors: string[];
        tableResults: { table: string; rows: number; error?: string }[];
    }> {
        const result = {
            success: true,
            tablesProcessed: 0,
            rowsCopied: 0,
            errors: [] as string[],
            tableResults: [] as { table: string; rows: number; error?: string }[],
        };

        // Log sync start to audit
        this.metadataService.auditLogRepository.create({
            action: 'data_sync_started',
            entityType: 'sync_run',
            connectionId: targetConnectionId,
            details: {
                type: 'dump_and_restore',
                sourceConnectionId,
                targetConnectionId,
                schema,
                tables: options.tables,
                truncateTarget: options.truncateTarget,
            },
        });

        const sourceConnection = this.connectionsService.findById(sourceConnectionId);
        const targetConnection = this.connectionsService.findById(targetConnectionId);
        const sourceConnector = await this.connectionsService.getConnector(sourceConnectionId);
        const targetConnector = await this.connectionsService.getConnector(targetConnectionId);

        const sourceEngine = sourceConnection.engine;
        const targetEngine = targetConnection.engine;

        // Get tables in dependency order
        let tables = await this.getTablesInDependencyOrder(sourceConnectionId, schema);

        // Filter to specific tables if provided
        if (options.tables && options.tables.length > 0) {
            const tableSet = new Set(options.tables);
            tables = tables.filter((t) => tableSet.has(t));
        }

        this.logger.log(
            `Dump & Restore: Processing ${tables.length} tables in order: ${tables.join(', ')}`
        );

        try {
            // Disable FK checks on target
            await this.setForeignKeyChecks(targetConnector, targetEngine, false);

            // Truncate tables in reverse order if requested
            if (options.truncateTarget) {
                const reverseTables = [...tables].reverse();
                for (const table of reverseTables) {
                    try {
                        const tableRef = quoteTableRef(schema, table, targetEngine);
                        if (targetEngine === 'postgres') {
                            await targetConnector.execute(`TRUNCATE TABLE ${tableRef} CASCADE`);
                        } else {
                            await targetConnector.execute(`TRUNCATE TABLE ${tableRef}`);
                        }
                        this.logger.debug(`Truncated ${schema}.${table}`);
                    } catch (error) {
                        const msg = `Failed to truncate ${schema}.${table}: ${error instanceof Error ? error.message : String(error)}`;
                        this.logger.warn(msg);
                        result.errors.push(msg);
                    }
                }
            }

            // Copy data for each table in dependency order
            for (const table of tables) {
                const tableResult = { table, rows: 0, error: undefined as string | undefined };

                try {
                    const sourceTableRef = quoteTableRef(schema, table, sourceEngine);
                    const targetTableRef = quoteTableRef(schema, table, targetEngine);

                    // Get table schema to know columns and their types
                    const tableSchema = await sourceConnector.getTableSchema(schema, table);
                    const columns = tableSchema.columns.map((c) => c.name);
                    const columnTypes = new Map(
                        tableSchema.columns.map((c) => [c.name, c.dataType])
                    );

                    // Fetch all data from source
                    const sourceData = await sourceConnector.query(
                        `SELECT * FROM ${sourceTableRef}`
                    );

                    if (sourceData.rows.length === 0) {
                        this.logger.debug(`No data in ${schema}.${table}`);
                        result.tableResults.push(tableResult);
                        result.tablesProcessed++;
                        continue;
                    }

                    // Insert data in batches
                    const batchSize = 100;
                    for (let i = 0; i < sourceData.rows.length; i += batchSize) {
                        const batch = sourceData.rows.slice(i, i + batchSize);

                        for (const row of batch) {
                            try {
                                // Serialize values, handling JSON columns properly
                                const values = columns.map((c) => {
                                    const dataType = columnTypes.get(c) || '';
                                    return serializeValueForInsert(row[c], dataType);
                                });
                                const placeholders = columns
                                    .map((_, idx) => getPlaceholder(idx + 1, targetEngine))
                                    .join(', ');
                                const quotedColumns = columns
                                    .map((c) => quoteIdentifier(c, targetEngine))
                                    .join(', ');

                                await targetConnector.execute(
                                    `INSERT INTO ${targetTableRef} (${quotedColumns}) VALUES (${placeholders})`,
                                    values
                                );
                                tableResult.rows++;
                            } catch (insertError) {
                                // Log but continue with other rows
                                this.logger.warn(
                                    `Failed to insert row in ${table}: ${insertError}`
                                );
                            }
                        }
                    }

                    result.rowsCopied += tableResult.rows;
                    this.logger.debug(`Copied ${tableResult.rows} rows to ${schema}.${table}`);
                } catch (error) {
                    tableResult.error = error instanceof Error ? error.message : String(error);
                    result.errors.push(`Table ${table}: ${tableResult.error}`);
                    result.success = false;
                }

                result.tableResults.push(tableResult);
                result.tablesProcessed++;
            }

            // Reset sequences/auto-increment for PostgreSQL
            if (targetEngine === 'postgres') {
                for (const table of tables) {
                    try {
                        const tableSchema = await targetConnector.getTableSchema(schema, table);
                        for (const col of tableSchema.columns) {
                            if (isAutoGeneratedColumn(col)) {
                                const tableRef = quoteTableRef(schema, table, targetEngine);
                                const colRef = quoteIdentifier(col.name, targetEngine);
                                await targetConnector.execute(
                                    `SELECT setval(pg_get_serial_sequence('${schema}.${table}', '${col.name}'), COALESCE((SELECT MAX(${colRef}) FROM ${tableRef}), 1), true)`
                                );
                            }
                        }
                    } catch (error) {
                        // Non-critical error, just log
                        this.logger.warn(`Failed to reset sequence for ${table}: ${error}`);
                    }
                }
            }
        } finally {
            // Re-enable FK checks
            await this.setForeignKeyChecks(targetConnector, targetEngine, true);
        }

        // Log sync completion to audit
        this.metadataService.auditLogRepository.create({
            action: result.errors.length > 0 ? 'data_sync_failed' : 'data_sync_completed',
            entityType: 'sync_run',
            connectionId: targetConnectionId,
            details: {
                type: 'dump_and_restore',
                sourceConnectionId,
                schema,
                success: result.success,
                tablesProcessed: result.tablesProcessed,
                rowsCopied: result.rowsCopied,
                errors: result.errors,
            },
        });

        return result;
    }

    /**
     * Helper to enable/disable foreign key checks
     */
    private async setForeignKeyChecks(
        connector: Awaited<ReturnType<typeof this.connectionsService.getConnector>>,
        engine: DatabaseEngine,
        enabled: boolean
    ): Promise<void> {
        try {
            if (engine === 'postgres') {
                // PostgreSQL: Use session_replication_role
                await connector.execute(
                    enabled
                        ? "SET session_replication_role = 'origin'"
                        : "SET session_replication_role = 'replica'"
                );
            } else if (engine === 'mysql' || engine === 'mariadb') {
                await connector.execute(`SET FOREIGN_KEY_CHECKS = ${enabled ? 1 : 0}`);
            } else if (engine === 'sqlite') {
                await connector.execute(`PRAGMA foreign_keys = ${enabled ? 'ON' : 'OFF'}`);
            }
        } catch (error) {
            this.logger.warn(`Failed to ${enabled ? 'enable' : 'disable'} FK checks: ${error}`);
        }
    }
}
